<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Business on Air on G</title>
    <link>http://air.googol.im/tags/business/index.xml</link>
    <description>Recent content in Business on Air on G</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh_CN</language>
    <copyright>All rights reserved - 2017</copyright>
    <atom:link href="http://air.googol.im/tags/business/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>试着解释大数据</title>
      <link>http://air.googol.im/post/explain-big-data/</link>
      <pubDate>Tue, 11 Mar 2014 21:01:03 +0800</pubDate>
      
      <guid>http://air.googol.im/post/explain-big-data/</guid>
      <description>&lt;p&gt;这篇blog本来是在ourcoders的一篇&lt;a href=&#34;http://ourcoders.com/thread/show/2671/#floor15&#34;&gt;回复&lt;/a&gt;。写完几天后，觉得还有必要总结留底，所以做了些修改，形成了这篇文章。&lt;/p&gt;

&lt;p&gt;我做大数据其实时间并不长，对大数据的理解也还处于很粗浅的阶段，欢迎大家讨论。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;大数据这事其实有两层意思：一层是单纯从业务上说，到底如何收集并有效利用数据做决策；另一层是指如何处理数据并完成决策所需要的数据支持。&lt;/p&gt;

&lt;p&gt;业务上利用数据做决策，是算法科学家或者现在所谓的大数据科学家，甚至是管理层和客户的事情。他们首先要了解运行的业务是什么，然后找出可以量化的关键点，再通过数据来检验这些量化指标，最终得出决策，听上去和程序员debug差不多。&lt;/p&gt;

&lt;p&gt;处理数据是公司的基础it架构，属于运维和开发的范畴，google的map/reduce，后来的hadoop都是在解决这一块的问题。&lt;/p&gt;

&lt;p&gt;一般来说，公司小的时候数据不多，用excel就能很好的处理。随着数据增加，使用数据库存储数据，配合脚本计算是常用的方法。如果业务很大，需要计算的数值变化频繁和数据量的增加，单点的数据库效率会变得越来越低，直到完全没法忍受。这时候就需要考虑使用mapreduce的分布式解决方案。这也是hadoop的真正用武之地。&lt;/p&gt;

&lt;p&gt;数据量会暴涨的一个主要原因，是互联网正在量化越来越多的行为，由此产生了越来越多的数据。以前只能通过抽样调查得到的数据（比如收视率，用户的使用习惯），现在可以通过各种方式直接拿到所有用户的数据。既然有数据了就要利用，所以现在企业用来分析的数据也不再是采样数据，而更多是全量数据。所以有人也会把现在的大数据称作全数据。&lt;/p&gt;

&lt;p&gt;讲个牛逼的八卦：美国80年代有家叫尼尔森的公司，专门做收视率调查。他们做法非常牛逼，会和家庭签协议，调查这个家庭的一些背景，并放一个与有线电视网联通的盒子在电视机旁边。这个盒子可不是小米盒子，而是个录音盒，目的在根据录音判断这家人看到了哪些广告。这事到这，只能说明当年大家想要收集一些数据都很辛苦，而且收集到的数据有很大的随机性。但是这事没完。后来全世界人民都非常开心的把自己的信息主动写在一个网站上，而尼尔森公司也看到这个机会，就和这家网站合作，取得了大量用户的背景信息（当然理论上是不能反查到个人的），并利用这些信息和自己的收视率数据合并，于是尼尔森公司就能更加准确地提供收视率了。这家网站，叫Facebook。&lt;/p&gt;

&lt;p&gt;这事可以说是数据上，从抽样数据转向全量数据的典型。现在各大网站利用cookie这些浏览信息暗中串通记录用户信息也不是什么秘密了，也一直有人说个人的行为在互联网上完全没法隐藏。既然公司买卖的都是全量数据，那么拿来做分析的当然也不会再仅限于抽样数据，也进入了全量数据处理的时代。&lt;/p&gt;

&lt;p&gt;大数据的架构，除了要解决使用单点数据库的性能，方便业务扩展时横向扩展系统的最大性能，另一方面也要考虑数据的提出者和使用者并不是程序员，而是对技术理解欠佳的决策层和科学家。从技术的发展脉络上来看，是让人家写c++/java（传统mapreduce），还是翻译更简单使用更广泛的sql（hive）？而hive是批处理模式不适合快速查询，于是spark是如何引入内存加速，而storm又是如何引入流来加快分析周期？aws又是如何提供hadoop集群来简化部署？&lt;/p&gt;

&lt;p&gt;最后试着用一句话总结一下：如果是公司层面思考大数据，更多应该关心如何拿到全量数据，如何才能从全量数据里拿到有效决策；而如果是工程层面思考大数据，就是如何搭起一套通用灵活的架构，来满足日益增长的分析业务。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>